{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a9ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd #data analysis\n",
    "from sklearn.model_selection import train_test_split #machine learning\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, BayesianRidge, LogisticRegression, LogisticRegressionCV\n",
    "import statsmodels.formula.api as sm #statistics and econometrics\n",
    "import matplotlib.pylab as plt #plotting library\n",
    "from dmba import regressionSummary, exhaustive_search\n",
    "from dmba import backward_elimination, forward_selection, stepwise_selection\n",
    "from dmba import adjusted_r2_score, AIC_score, BIC_score\n",
    "import dmba\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c25ad9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'student-mat.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent-mat.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent-por.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1,df2],axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'student-mat.csv'"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('student-mat.csv')\n",
    "df2 = pd.read_csv('student-por.csv')\n",
    "df = pd.concat([df1,df2],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column Weekly Conumption - AlWeekly_Cons as the sum of 'Dalc' & 'Walc'\n",
    "df['AlWeekly_Cons'] = round((df['Dalc']+ df['Walc'])/2,0) # 1- 10\n",
    "df['AlWeekly_Sum'] = round((df['Dalc']+ df['Walc']),0) # 1- 10\n",
    "\n",
    "#changed this to average as 1-10 spread the prediction too wide and didn't give the most conclusive prediction.\n",
    "\n",
    "# Drop column G1 and G2 as we only need G3 - final grade and drop 'Dalc' and 'Walc' because we have the weekly sum\n",
    "df = df.drop(columns = ['G1','G2','Dalc','Walc'],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01923bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Success column with two value yes and no based on the G3 column\n",
    "# We choose threshold of 11 to decide whether a student is academically successful or not (1 point bigger than average which is 10)\n",
    "df['Success'] = df.apply(lambda x: 'yes' if x['G3'] >= 11 else 'no',axis = 1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf016fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('student.csv')\n",
    "df\n",
    "#df['Medu'] = df['Medu'].astype('category')\n",
    "#df['Fedu'] = df['Fedu'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check with running everything. \n",
    "predictors = ['famsize', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'guardian', 'famsup',\n",
    "              'Pstatus'] #select the desired columns\n",
    "#predictors = ['famsize', 'Medu', 'Mjob', 'Fjob', 'guardian', 'famsup',\n",
    "              #'Pstatus'] #remove FEDU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fefaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'AlWeekly_Cons' #define the outcome variable\n",
    "# partition data\n",
    "# Check indexing\n",
    "#make MEDU AND FEDU AS CATEGORICALS\n",
    "\n",
    "X = pd.get_dummies(df[predictors], drop_first=True) #convert categorical variables into dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc85ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[outcome] #assign the outcome to y\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1) #partition the data to train/test\n",
    "\n",
    "student_lm = LinearRegression() #assign the linear regression model\n",
    "student_lm.fit(train_X, train_y) #linear regeression on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print coefficients\n",
    "print('intercept ', student_lm.intercept_)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(pd.DataFrame({'Predictor': X.columns, 'coefficient': student_lm.coef_}))\n",
    "\n",
    "# print performance measures\n",
    "regressionSummary(train_y, student_lm.predict(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ce027",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = student_lm.predict(train_X) #prediction performance on the training set \n",
    "\n",
    "print('adjusted r2 : ', adjusted_r2_score(train_y, pred_y, student_lm))\n",
    "print('AIC : ', AIC_score(train_y, pred_y, student_lm))\n",
    "print('BIC : ', BIC_score(train_y, pred_y, student_lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604261e3",
   "metadata": {},
   "source": [
    "As this prediction gives continuous,decimal value, for an integer/categorical outcome (1,2,3,4,5). I'm applying rounding to the predicted value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use predict() to make predictions on a new set\n",
    "student_lm_pred = student_lm.predict(valid_X)\n",
    "\n",
    "# Round the predictions\n",
    "student_lm_pred_rounded = np.round(student_lm_pred)\n",
    "# Clip values to be within the range [1, 5]\n",
    "student_lm_pred_clipped = np.clip(student_lm_pred_rounded, 1, 5)\n",
    "\n",
    "result = pd.DataFrame({'Predicted': student_lm_pred_clipped, 'Actual': valid_y,\n",
    "                       'Residual': valid_y - student_lm_pred_clipped}) #dataframe with three columns\n",
    "print(result)\n",
    "\n",
    "# Compute common accuracy measures\n",
    "regressionSummary(valid_y, student_lm_pred_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on a new set\n",
    "student_lm_pred = student_lm.predict(valid_X)\n",
    "\n",
    "# Round the predictions\n",
    "student_lm_pred_rounded = np.round(student_lm_pred)\n",
    "# Clip values to be within the range [1, 5]\n",
    "student_lm_pred_clipped = np.clip(student_lm_pred_rounded, 1, 5)\n",
    "\n",
    "# Compute residuals based on the rounded and clipped predictions\n",
    "all_residuals = valid_y - student_lm_pred_clipped\n",
    "\n",
    "# Determine the percentage of datapoints with a residual in [-1406, 1406]\n",
    "print(len(all_residuals[(all_residuals > -1406) & (all_residuals < 1406)]) / len(all_residuals))\n",
    "\n",
    "# Plotting the residuals\n",
    "ax = pd.DataFrame({'Residuals': all_residuals}).hist(bins=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10065814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lasso = Pipeline([\n",
    "    ['normalize', StandardScaler()],\n",
    "    ['model', Lasso(alpha=1)],\n",
    "])\n",
    "lasso.fit(train_X, train_y)\n",
    "regressionSummary(valid_y, lasso.predict(valid_X))\n",
    "\n",
    "lasso_cv = Pipeline([\n",
    "    ['normalize', StandardScaler()],\n",
    "    ['model', LassoCV(cv=5)],\n",
    "])\n",
    "lasso_cv.fit(train_X, train_y)\n",
    "regressionSummary(valid_y, lasso_cv.predict(valid_X))\n",
    "print('Lasso-CV chosen regularization: ', lasso_cv['model'].alpha_)\n",
    "print(lasso_cv['model'].coef_)\n",
    "\n",
    "ridge = Pipeline([\n",
    "    ['normalize', StandardScaler()],\n",
    "    ['model', Ridge(alpha=1)],\n",
    "])\n",
    "ridge.fit(train_X, train_y)\n",
    "regressionSummary(valid_y, ridge.predict(valid_X))\n",
    "\n",
    "bayesianRidge = Pipeline([\n",
    "    ['normalize', StandardScaler()],\n",
    "    ['model', BayesianRidge()],\n",
    "])\n",
    "bayesianRidge.fit(train_X, train_y)\n",
    "regressionSummary(valid_y, bayesianRidge.predict(valid_X))\n",
    "print('Bayesian ridge chosen regularization: ',\n",
    "      bayesianRidge['model'].lambda_ / bayesianRidge['model'].alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegression = Pipeline([\n",
    "    ['normalize', StandardScaler()],\n",
    "    ['model', LinearRegression()],\n",
    "])\n",
    "linearRegression.fit(train_X, train_y)\n",
    "regressionSummary(valid_y, linearRegression.predict(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'features': train_X.columns,\n",
    "    'linear regression': linearRegression['model'].coef_,\n",
    "    'lassoCV': lasso_cv['model'].coef_,\n",
    "    'bayesianRidge': bayesianRidge['model'].coef_,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a linear regression of Price on the remaining 11 predictors in the training set\n",
    "train_df = train_X.join(train_y)\n",
    "\n",
    "predictors = train_X.columns\n",
    "formula = 'AlWeekly_Cons ~ ' + ' + '.join(predictors)\n",
    "\n",
    "student_lm = sm.ols(formula=formula, data=train_df).fit()\n",
    "print(student_lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490dcfac",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded18bb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Adding a column to split low and high alcohol consumption\n",
    "df['Alcohol_Consumption_High'] = df['AlWeekly_Sum'].apply(lambda x: 0 if x < 4 else 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_LR = df['Alcohol_Consumption_High'] #define the outcome variable\n",
    "\n",
    "#classes = ['low', 'high']\n",
    "\n",
    "# split into training and validation\n",
    "train_X, valid_X, train_y_LR, valid_y_LR = train_test_split(X, y_LR, test_size=0.4,\n",
    "                                                      random_state=1)\n",
    "\n",
    "logit_red = LogisticRegressionCV(penalty=\"l1\", solver='liblinear', cv=5)\n",
    "logit_red.fit(train_X, train_y_LR)\n",
    "\n",
    "pd.set_option('display.width', 100)\n",
    "print('regularization', logit_red.C_)\n",
    "print('intercept ', logit_red.intercept_[0])\n",
    "print(pd.DataFrame({'coeff': logit_red.coef_[0]}, index=X.columns).transpose())\n",
    "pd.reset_option('display.width')\n",
    "print('AIC', AIC_score(valid_y_LR, logit_red.predict(valid_X), df=len(train_X.columns) + 1))\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "classificationSummary(valid_y_LR, logit_red.predict(valid_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a43f6",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c995fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "rf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "df = pd.DataFrame({'feature': train_X.columns, 'importance': importances, 'std': std})\n",
    "df = df.sort_values('importance', ascending=False)\n",
    "print(df)\n",
    "\n",
    "ax = df.plot(kind='barh', xerr='std', x='feature', legend=False)\n",
    "ax.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmba import plotDecisionTree, classificationSummary, regressionSummary\n",
    "\n",
    "classificationSummary(valid_y, rf.predict(valid_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08489a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict Success"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
